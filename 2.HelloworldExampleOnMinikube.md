## Helloworld Example on Knative

### Deploying the helloworld Service on Knative
We will deploy a simple application on Knative using an existing Container Image. 

* Create a `service.yaml` with following contents

```
$ cat helloworld/service.yaml 
apiVersion: serving.knative.dev/v1alpha1 # Current version of Knative
kind: Service
metadata:
  name: helloworld # The name of the app
  namespace: myproject # The namespace the app will use
spec:
  runLatest:
    configuration:
      revisionTemplate:
        spec:
          container:
            image: docker.io/veermuchandi/welcome # The URL to the image of the app
```

Note the name of the `service`, `namespace` and `image`. Knative service encompasses a bunch of primitives i.e, `configuration`, `revision` and `route`. 

* Let us create Knative service using the above definition
```
$ kubectl apply -f helloworld/service.yaml 
service.serving.knative.dev/helloworld created
```

* Note the deploymet pod running in `myproject` namespace

```
$ kubectl get po -n myproject                                                                                           
NAME                                           READY     STATUS    RESTARTS   AGE
helloworld-00001-deployment-84d44f6f4f-fhzrt   2/2       Running   0          32s

```

* Note the services created. One of them is a kubernetes service (`helloworld-00001-service`) and the other one (`helloworld`) is the knative service defined as `service.serving.knative.dev`.

```
$ kubectl get service -n myproject
NAME                       TYPE           CLUSTER-IP      EXTERNAL-IP                                           PORT(S)           AGE
helloworld                 ExternalName   <none>          istio-ingressgateway.istio-system.svc.cluster.local   <none>            16s
helloworld-00001-service   ClusterIP      172.30.62.166   <none>                                                80/TCP,9090/TCP   26s
```

* Also look at configuration created by this service.  `configuration.serving.knative.dev`

```
$ kubectl get configuration.serving.knative.dev -n myproject
NAME         CREATED AT
helloworld   10m

$ kubectl get configuration.serving.knative.dev -n myproject -o yaml
apiVersion: v1
items:
- apiVersion: serving.knative.dev/v1alpha1
  kind: Configuration
  metadata:
    creationTimestamp: 2019-03-21T14:35:05Z
    generation: 1
    labels:
      serving.knative.dev/route: helloworld
      serving.knative.dev/service: helloworld
    name: helloworld
    namespace: myproject
    ownerReferences:
    - apiVersion: serving.knative.dev/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: Service
      name: helloworld
      uid: 84930f68-4be6-11e9-8135-eaf585462a4e
    resourceVersion: "14996"
    selfLink: /apis/serving.knative.dev/v1alpha1/namespaces/myproject/configurations/helloworld
    uid: 8495b858-4be6-11e9-8135-eaf585462a4e
  spec:
    generation: 1
    revisionTemplate:
      metadata:
        creationTimestamp: null
      spec:
        container:
          image: docker.io/veermuchandi/welcome
          name: ""
          resources: {}
        timeoutSeconds: 300
  status:
    conditions:
    - lastTransitionTime: 2019-03-21T14:35:14Z
      severity: Error
      status: "True"
      type: Ready
    latestCreatedRevisionName: helloworld-00001
    latestReadyRevisionName: helloworld-00001
    observedGeneration: 1
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
```

* `Configuration` refers to a specific revision. So let us look at the `revision` 

```
$ kubectl get revision.serving.knative.dev -n myproject
NAME               SERVICE NAME               READY     REASON
helloworld-00001   helloworld-00001-service   True 

$ kubectl get revision.serving.knative.dev -n myproject -o yaml
apiVersion: v1
items:
- apiVersion: serving.knative.dev/v1alpha1
  kind: Revision
  metadata:
    annotations:
      serving.knative.dev/lastPinned: "1553178915"
    creationTimestamp: 2019-03-21T14:35:05Z
    generation: 1
    labels:
      serving.knative.dev/configuration: helloworld
      serving.knative.dev/configurationGeneration: "1"
      serving.knative.dev/configurationMetadataGeneration: "1"
      serving.knative.dev/service: helloworld
    name: helloworld-00001
    namespace: myproject
    ownerReferences:
    - apiVersion: serving.knative.dev/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: Configuration
      name: helloworld
      uid: 8495b858-4be6-11e9-8135-eaf585462a4e
    resourceVersion: "15679"
    selfLink: /apis/serving.knative.dev/v1alpha1/namespaces/myproject/revisions/helloworld-00001
    uid: 8498f9ec-4be6-11e9-8135-eaf585462a4e
  spec:
    container:
      image: docker.io/veermuchandi/welcome
      name: ""
      resources: {}
    generation: 1
    timeoutSeconds: 300
  status:
    conditions:
    - lastTransitionTime: 2019-03-21T14:36:27Z
      message: The target is not receiving traffic.
      reason: NoTraffic
      severity: Info
      status: "False"
      type: Active
    - lastTransitionTime: 2019-03-21T14:35:05Z
      severity: Error
      status: "True"
      type: BuildSucceeded
    - lastTransitionTime: 2019-03-21T14:35:15Z
      severity: Error
      status: "True"
      type: ContainerHealthy
    - lastTransitionTime: 2019-03-21T14:35:15Z
      severity: Error
      status: "True"
      type: Ready
    - lastTransitionTime: 2019-03-21T14:35:15Z
      severity: Error
      status: "True"
      type: ResourcesAvailable
    imageDigest: index.docker.io/veermuchandi/welcome@sha256:db4d49ca6ab825c013cb076a2824947c5378b845206cc29c6fd245744ffd35fb
    logUrl: |
      http://localhost:8001/api/v1/namespaces/knative-monitoring/services/kibana-logging/proxy/app/kibana#/discover?_a=(query:(match:(kubernetes.labels.knative-dev%2FrevisionUID:(query:'8498f9ec-4be6-11e9-8135-eaf585462a4e',type:phrase))))
    serviceName: helloworld-00001-service
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

```

* Service creation also results in a Knative `route.serving.knative.dev` as seen below

```
$ kubectl get route.serving.knative.dev -n myproject
NAME         DOMAIN                             READY     REASON
helloworld   helloworld.myproject.example.com   True 

$ kubectl get route.serving.knative.dev helloworld -n myproject -o yaml
apiVersion: serving.knative.dev/v1alpha1
kind: Route
metadata:
  creationTimestamp: 2019-03-21T14:35:05Z
  generation: 1
  labels:
    serving.knative.dev/service: helloworld
  name: helloworld
  namespace: myproject
  ownerReferences:
  - apiVersion: serving.knative.dev/v1alpha1
    blockOwnerDeletion: true
    controller: true
    kind: Service
    name: helloworld
    uid: 84930f68-4be6-11e9-8135-eaf585462a4e
  resourceVersion: "15004"
  selfLink: /apis/serving.knative.dev/v1alpha1/namespaces/myproject/routes/helloworld
  uid: 8497f2aa-4be6-11e9-8135-eaf585462a4e
spec:
  generation: 1
  traffic:
  - configurationName: helloworld
    percent: 100
status:
  address:
    hostname: helloworld.myproject.svc.cluster.local
  conditions:
  - lastTransitionTime: 2019-03-21T14:35:15Z
    severity: Error
    status: "True"
    type: AllTrafficAssigned
  - lastTransitionTime: 2019-03-21T14:35:15Z
    severity: Error
    status: "True"
    type: IngressReady
  - lastTransitionTime: 2019-03-21T14:35:15Z
    severity: Error
    status: "True"
    type: Ready
  domain: helloworld.myproject.example.com
  domainInternal: helloworld.myproject.svc.cluster.local
  traffic:
  - percent: 100
    revisionName: helloworld-00001

```


The route above refers to a specific configuration `configurationName: helloworld`. It also has a domain name `helloworld.myproject.example.com`. You can get this value directly running `kubectl get route helloworld -n myproject -o jsonpath={.status.domain}`

* Save the application URL 

```
$ export URL=$(kubectl get route.serving.knative.dev helloworld -n myproject -o jsonpath={.status.domain})

$ echo $URL
helloworld.myproject.example.com
```

* In order to access this application you need the ingress IP and port number.  Note the IP Address of your minikube cluster and the port number `knative-ingressgateway` is exposed on as shown below.

```
$ minikube ip
192.168.64.44

$ kubectl get svc -n istio-system | grep knative
knative-ingressgateway     NodePort    10.109.207.140   <none>        80:32380/TCP,443:32390/TCP,32400:32400/TCP                            21m
```
> For **Minikube**: When looking up the IP address to use for accessing your app, you need to look up the NodePort for the knative-ingressgateway as well as the IP address used for Minikube

```
$ export IP_ADDRESS=$(minikube ip):$(kubectl get svc knative-ingressgateway -n istio-system -o 'jsonpath={.spec.ports[?(@.port==80)].nodePort}')
```

> For **Minishift**: Since we are using NodePort approach with minishift, you will find the IPAddress of the node and the NodePort for the knative-ingressgateway

```
$ export IP_ADDRESS=$(oc get node -o 'jsonpath={.items[0].status.addresses[0].address}'):$(oc get svc knative-ingressgateway -n istio-system -o 'jsonpath={.spec.ports[?(@.port==80)].nodePort}')
```


```
$ echo $IP_ADDRESS
192.168.64.44:32380
```


### Test the Application

* Before we test the application, if you leave the service untouched for a little while, you'll notice that no pods are running in the namespace `myproject` as seen below.

```
$ kubectl get po -n myproject
No resources found.
```

* Now test the URL running a curl. There would be a few seconds delay when you curl for the first time.

```
$ curl -H "Host: ${URL}" http://${IP_ADDRESS}
 Welcome to OpenShift 3 !!!

```
* Now if you check the pods, you will see a pod running. The delay in the response was because this pod had to be started on the fly when the request came in. 

```
$ kubectl get po -n myproject
NAME                                           READY     STATUS    RESTARTS   AGE
helloworld-00001-deployment-84d44f6f4f-nlx2l   2/2       Running   0          8s
```

### So how does it work?

Knative also runs an autoscaler for your service. The autoscaler brings up your pods when a request hits the service. It also automatically brings down the pod when there is no workload for sometime. Check the pods in the `knative-serving` namespace now to find it as you can see from the events in the description. 

```
$ kubectl get po -n knative-serving
NAME                          READY     STATUS    RESTARTS   AGE
activator-79ccd49498-2lv8l    2/2       Running   0          25m
autoscaler-7c99f67d94-4h6gx   2/2       Running   0          25m
controller-97f5b6d66-4v2z8    1/1       Running   0          25m
webhook-59b8f68bcf-6l2n6      1/1       Running   0          25m
```

Look at the autoscaler pod's logs by running `kubectl logs -f autoscaler-7c99f67d94-4h6gx -n knative-serving` (in your case the pod name will be different)

Specially look for logs from `kpa/kpa_scaler.go` 

```
{"level":"info","ts":"2019-03-21T18:36:23.777Z","logger":"autoscaler.kpa-class-podautoscaler-controller","caller":"kpa/kpa_scaler.go:178","msg":"Scaling from 0 to 1","commit":"4d198db","knative.dev/controller":"kpa-class-podautoscaler-controller","knative.dev/key":"myproject/helloworld-00001"}
...
...

{"level":"info","ts":"2019-03-21T18:38:15.026Z","logger":"autoscaler.kpa-class-podautoscaler-controller","caller":"kpa/kpa_scaler.go:178","msg":"Scaling from 1 to 0","commit":"4d198db","knative.dev/controller":"kpa-class-podautoscaler-controller","knative.dev/key":"myproject/helloworld-00001"}
...
...

```
You will see the pod scale up from 0 to 1 and then eventually from 1 to 0 when there is no traffic.

Also note `scale-to-zero-grace-period`. This is the amount of time an inactive revision is left running before scaling down to 0.

```
$ kubectl get cm -n knative-serving config-autoscaler -o yaml | grep "^\s scale-to-zero-grace"
  scale-to-zero-grace-period: 30s
```


### Tearing down the service

Run the following command to delete the service from the namespace

```
$ kubectl delete -f helloworld/service.yaml -n myproject
service.serving.knative.dev "helloworld" deleted
```







