## Helloworld Example on Knative

### Deploying the helloworld Service on Knative
We will deploy a simple application on Knative using an existing Container Image. 

* Create a `service.yaml` with following contents

```
$ cat helloworld/service.yaml 
apiVersion: serving.knative.dev/v1alpha1 # Current version of Knative
kind: Service
metadata:
  name: helloworld # The name of the app
  namespace: myproject # The namespace the app will use
spec:
  runLatest:
    configuration:
      revisionTemplate:
        spec:
          container:
            image: docker.io/veermuchandi/welcome # The URL to the image of the app
```

Note the name of the `service`, `namespace` and `image`. Knative service encompasses a bunch of primitives i.e, `configuration`, `revision` and `route`. 

* Let us create Knative service using the above definition
```
$ kubectl apply -f helloworld/service.yaml 
service.serving.knative.dev/helloworld created
```

* Note the deploymet pod running in `myproject` namespace

```
$ kubectl get po -n myproject                                                                                           
NAME                                           READY     STATUS    RESTARTS   AGE
helloworld-00001-deployment-84d44f6f4f-fhzrt   2/2       Running   0          32s

```

* Note the services created. One of them is a kubernetes service (`helloworld-00001-service`) and the other one (`helloworld`) is the knative service defined as `service.serving.knative.dev`.

```
$ kubectl get service -n myproject
NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
helloworld                 ClusterIP   10.105.229.61   <none>        80/TCP         4m
helloworld-00001-service   NodePort    10.97.150.9     <none>        80:31705/TCP   4m
```

* Also look at configuration created by this service.  `configuration.serving.knative.dev`

```
$ kubectl get configuration.serving.knative.dev -n myproject
NAME         CREATED AT
helloworld   10m

$ kubectl get configuration.serving.knative.dev -n myproject -o yaml
apiVersion: v1
items:
- apiVersion: serving.knative.dev/v1alpha1
  kind: Configuration
  metadata:
    clusterName: ""
    creationTimestamp: 2018-09-04T16:52:42Z
    generation: 1
    labels:
      serving.knative.dev/route: helloworld
      serving.knative.dev/service: helloworld
    name: helloworld
    namespace: myproject
    ownerReferences:
    - apiVersion: serving.knative.dev/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: Service
      name: helloworld
      uid: f0ae7db8-b062-11e8-bf4a-babfeb3fe64d
    resourceVersion: "6215"
    selfLink: /apis/serving.knative.dev/v1alpha1/namespaces/myproject/configurations/helloworld
    uid: f0b0a45d-b062-11e8-bf4a-babfeb3fe64d
  spec:
    generation: 1
    revisionTemplate:
      metadata:
        creationTimestamp: null
      spec:
        concurrencyModel: Multi
        container:
          image: docker.io/veermuchandi/welcome
          name: ""
          resources: {}
  status:
    conditions:
    - lastTransitionTime: 2018-09-04T16:57:46Z
      message: 'Revision "helloworld-00001" failed with message: "".'
      reason: RevisionFailed
      status: "False"
      type: Ready
    latestCreatedRevisionName: helloworld-00001
    latestReadyRevisionName: helloworld-00001
    observedGeneration: 1
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
```

* `Configuration` refers to a specific revision. So let us look at the `revision` 

```
$ kubectl get revision.serving.knative.dev -n myproject
NAME               CREATED AT
helloworld-00001   24m

$ kubectl get revision.serving.knative.dev -n myproject -o yaml
apiVersion: v1
items:
- apiVersion: serving.knative.dev/v1alpha1
  kind: Revision
  metadata:
    annotations:
      serving.knative.dev/configurationGeneration: "1"
    clusterName: ""
    creationTimestamp: 2018-09-04T16:52:42Z
    generation: 1
    labels:
      serving.knative.dev/configuration: helloworld
    name: helloworld-00001
    namespace: myproject
    ownerReferences:
    - apiVersion: serving.knative.dev/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: Configuration
      name: helloworld
      uid: f0b0a45d-b062-11e8-bf4a-babfeb3fe64d
    resourceVersion: "6214"
    selfLink: /apis/serving.knative.dev/v1alpha1/namespaces/myproject/revisions/helloworld-00001
    uid: f0b2686f-b062-11e8-bf4a-babfeb3fe64d
  spec:
    concurrencyModel: Multi
    container:
      image: docker.io/veermuchandi/welcome
      name: ""
      resources: {}
    generation: 2
    servingState: Reserve
  status:
    conditions:
    - lastTransitionTime: 2018-09-04T16:57:46Z
      reason: Updating
      status: Unknown
      type: ResourcesAvailable
    - lastTransitionTime: 2018-09-04T16:57:46Z
      reason: Updating
      status: Unknown
      type: ContainerHealthy
    - lastTransitionTime: 2018-09-04T16:57:46Z
      reason: Inactive
      status: "False"
      type: Ready


```

* Service creation also results in a Knative `route.serving.knative.dev` as seen below

```
$ kubectl get route.serving.knative.dev -n myproject
NAME         CREATED AT
helloworld   26m

$ kubectl get route.serving.knative.dev helloworld -n myproject -o yaml
apiVersion: serving.knative.dev/v1alpha1
kind: Route
metadata:
  clusterName: ""
  creationTimestamp: 2018-09-04T16:52:42Z
  generation: 1
  labels:
    serving.knative.dev/service: helloworld
  name: helloworld
  namespace: myproject
  ownerReferences:
  - apiVersion: serving.knative.dev/v1alpha1
    blockOwnerDeletion: true
    controller: true
    kind: Service
    name: helloworld
    uid: f0ae7db8-b062-11e8-bf4a-babfeb3fe64d
  resourceVersion: "5495"
  selfLink: /apis/serving.knative.dev/v1alpha1/namespaces/myproject/routes/helloworld
  uid: f0b17eb0-b062-11e8-bf4a-babfeb3fe64d
spec:
  generation: 1
  traffic:
  - configurationName: helloworld
    percent: 100
status:
  conditions:
  - lastTransitionTime: 2018-09-04T16:52:46Z
    status: "True"
    type: AllTrafficAssigned
  - lastTransitionTime: 2018-09-04T16:52:46Z
    status: "True"
    type: Ready
  domain: helloworld.myproject.example.com
  domainInternal: helloworld.myproject.svc.cluster.local
  traffic:
  - configurationName: helloworld
    percent: 100
    revisionName: helloworld-00001

```


The route above refers to a specific configuration `configurationName: helloworld`. It also has a domain name `helloworld.myproject.example.com`. You can get this value directly running `kubectl get route helloworld -n myproject -o jsonpath={.status.domain}`

* Save the application URL 

```
$ export URL=$(kubectl get route.serving.knative.dev helloworld -n myproject -o jsonpath={.status.domain})

$ echo $URL
helloworld.myproject.example.com
```

* In order to access this application you need the ingress IP and port number.  Note the IP Address of your minikube cluster and the port number `knative-ingressgateway` is exposed on as shown below.

```
$ minikube ip
192.168.64.44

$ kubectl get svc -n istio-system | grep knative
knative-ingressgateway     NodePort    10.109.207.140   <none>        80:32380/TCP,443:32390/TCP,32400:32400/TCP                            21m
```
> For **Minikube**: When looking up the IP address to use for accessing your app, you need to look up the NodePort for the knative-ingressgateway as well as the IP address used for Minikube

```
$ export IP_ADDRESS=$(minikube ip):$(kubectl get svc knative-ingressgateway -n istio-system -o 'jsonpath={.spec.ports[?(@.port==80)].nodePort}')
```

> For **Minishift**: Since we are using NodePort approach with minishift, you will find the IPAddress of the node and the NodePort for the knative-ingressgateway

```
$ export IP_ADDRESS=$(oc get node -o 'jsonpath={.items[0].status.addresses[0].address}'):$(oc get svc knative-ingressgateway -n istio-system -o 'jsonpath={.spec.ports[?(@.port==80)].nodePort}')
```


```
$ echo $IP_ADDRESS
192.168.64.44:32380
```


### Test the Application

* Before we test the application, if you leave the service untouched for a little while, you'll notice that no pods are running in the namespace `myproject` as seen below.

```
$ kubectl get po -n myproject
No resources found.
```

* Now test the URL running a curl. There would be a few seconds delay when you curl for the first time.

```
$ curl -H "Host: ${URL}" http://${IP_ADDRESS}
 Welcome to OpenShift 3 !!!

```
* Now if you check the pods, you will see a pod running. The delay in the response was because this pod had to be started on the fly when the request came in. 

```
$ kubectl get po -n myproject
NAME                                           READY     STATUS    RESTARTS   AGE
helloworld-00001-deployment-84d44f6f4f-nlx2l   2/2       Running   0          8s
```

### So how does it work?

Knative also runs an autoscaler for your service. The autoscaler brings up your pods when a request hits the service. It also automatically brings down the pod when there is no workload for sometime. Check the pods in the `knative-serving` namespace now to find it as you can see from the events in the description. 

```
$ kubectl get po -n knative-serving
NAME                                           READY     STATUS    RESTARTS   AGE
activator-847cf57479-kbfr8                     2/2       Running   0          2h
controller-5d7f46bfd6-22h6t                    1/1       Running   0          2h
helloworld-00001-autoscaler-5d545994dc-4tp95   2/2       Running   0          48s
webhook-7f8ddf4499-zxv7q                       1/1       Running   0          2h
```

This autoscaler pod is started as a replicaset as shown below and is controlled by knative serving primitives by changing the desired number of replicas on the fly. 

```
$ kubectl get rs -n knative-serving
NAME                                     DESIRED   CURRENT   READY     AGE
activator-847cf57479                     1         1         1         2h
controller-5d7f46bfd6                    1         1         1         2h
helloworld-00001-autoscaler-5d545994dc   1         1         1         1h
webhook-7f8ddf4499                       1         1         1         2h

  
$ kubectl get deployment -n knative-serving
NAME                          DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
activator                     1         1         1            1           2h
controller                    1         1         1            1           2h
helloworld-00001-autoscaler   0         0         0            0           1h
webhook                       1         1         1            1           2h

$ kubectl describe deployment helloworld-00001-autoscaler -n knative-serving
Name:                   helloworld-00001-autoscaler
Namespace:              knative-serving
CreationTimestamp:      Tue, 04 Sep 2018 12:52:43 -0400
Labels:                 app=helloworld-00001
                        serving.knative.dev/configuration=helloworld
                        serving.knative.dev/revision=helloworld-00001
                        serving.knative.dev/revisionUID=f0b2686f-b062-11e8-bf4a-babfeb3fe64d
Annotations:            deployment.kubernetes.io/revision=1
                        serving.knative.dev/configurationGeneration=1
Selector:               app=helloworld-00001,serving.knative.dev/configuration=helloworld,serving.knative.dev/revision=helloworld-00001,serving.knative.dev/revisionUID=f0b2686f-b062-11e8-bf4a-babfeb3fe64d
Replicas:               0 desired | 0 updated | 0 total | 0 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           app=helloworld-00001
                    serving.knative.dev/autoscaler=helloworld-00001-autoscaler
                    serving.knative.dev/configuration=helloworld
                    serving.knative.dev/revision=helloworld-00001
                    serving.knative.dev/revisionUID=f0b2686f-b062-11e8-bf4a-babfeb3fe64d
  Annotations:      serving.knative.dev/configurationGeneration=1
                    sidecar.istio.io/inject=true
  Service Account:  autoscaler
  Containers:
   autoscaler:
    Image:      gcr.io/knative-releases/github.com/knative/serving/cmd/autoscaler@sha256:2a9931ab1cbe6efeec43a47517163622002e0dc2ea5d6ffa0d0e546c2c94b841
    Port:       8080/TCP
    Host Port:  0/TCP
    Args:
      -concurrencyModel=Multi
    Requests:
      cpu:  25m
    Environment:
      SERVING_NAMESPACE:        myproject
      SERVING_DEPLOYMENT:       helloworld-00001-deployment
      SERVING_CONFIGURATION:    helloworld
      SERVING_REVISION:         helloworld-00001
      SERVING_AUTOSCALER_PORT:  8080
    Mounts:
      /etc/config-autoscaler from config-autoscaler (rw)
      /etc/config-logging from config-logging (rw)
  Volumes:
   config-autoscaler:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      config-autoscaler
    Optional:  false
   config-logging:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      config-logging
    Optional:  false
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   helloworld-00001-autoscaler-5d545994dc (0/0 replicas created)
Events:
  Type    Reason             Age              From                   Message
  ----    ------             ----             ----                   -------
  Normal  ScalingReplicaSet  2h (x5 over 4h)  deployment-controller  Scaled up replica set helloworld-00001-autoscaler-5d545994dc to 1
  Normal  ScalingReplicaSet  2h (x5 over 4h)  deployment-controller  Scaled down replica set helloworld-00001-autoscaler-5d545994dc to 0
```

### Tearing down the service

Run the following command to delete the service from the namespace

```
$ kubectl delete -f helloworld/service.yaml -n myproject
service.serving.knative.dev "helloworld" deleted
```

Check the namespace `myproject` to see it is empty

```
$ kubectl get all -n myproject
No resources found.
```

Check the resources in the `knative-serving` namespace

```
$ kubectl get rs -n knative
No resources found.

$ kubectl get deployment -n knative
No resources found.
```

If you are running on minishift, as of now, you'll have to delete all the objects yourself to clean up the project by running the following:

```
oc project myproject
kubectl delete service.serving.knative.dev --all
kubectl delete configuration.serving.knative.dev --all
kubectl delete revision.serving.knative.dev --all
kubectl delete route.serving.knative.dev --all
kubectl delete builds.build.knative.dev --all
kubectl delete service --all
kubectl delete rs --all
kubectl delete po --all
kubectl delete deployment $(kubectl get deployments -n knative-serving | grep autoscaler | awk '{print $1}') -n knative-serving
kubectl delete rs $(kubectl get rs -n knative-serving | grep autoscaler | awk '{print $1}') -n knative-serving
kubectl delete po $(kubectl get po -n knative-serving | grep autoscaler | awk '{print $1}') -n knative-serving

```







